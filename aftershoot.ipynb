{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmll8COLpkpPoSToiHU3Y0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SouravSangralSrv/SouravSangralSrv/blob/main/aftershoot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the paths\n",
        "zip_path = '/content/aftershoot.zip' # The name of your existing ZIP file\n",
        "# ðŸš¨ FIX: Change the destination path to a NEW folder name\n",
        "extract_path = '/content/aftershoot_extracted'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "# This is where the contents will go\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Open the zip file and extract all contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted successfully to: {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icHPYbM0sBEe",
        "outputId": "0f0894cf-0b42-417d-ece9-e6ca28bf9b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted successfully to: /content/aftershoot_extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPk_H4xBodJ_",
        "outputId": "2c34c8c8-45fe-4792-8cc3-b972fdc66abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WHITE BALANCE PREDICTOR - Delta-based approach\n",
            "============================================================\n",
            "\n",
            "Starting training...\n",
            "Using 7 features: ['aperture', 'flashFired', 'focalLength', 'isoSpeedRating', 'shutterSpeed', 'currTemp', 'currTint']\n",
            "\n",
            "ðŸŽ¯ Using DELTA prediction (change from current values)\n",
            "  Removing outliers outside: Temp [-2908, 2470], Tint [-21, 19]\n",
            "Loading images...\n",
            "Detected image extension: .tif\n",
            "Loaded 500/2538 images\n",
            "Loaded 1000/2538 images\n",
            "Loaded 1500/2538 images\n",
            "Loaded 2000/2538 images\n",
            "Loaded 2500/2538 images\n",
            "Successfully loaded 2538 images\n",
            "\n",
            "Delta statistics:\n",
            "  Temp delta range: [-2908, 2470]\n",
            "  Temp delta mean: -313.8, std: 921.8\n",
            "  Tint delta range: [-21, 19]\n",
            "  Tint delta mean: -0.8, std: 7.8\n",
            "Training image feature extractor...\n",
            "Extracting image features...\n",
            "Training fusion model...\n",
            "Epoch 1/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 349.9665 - mae: 349.9665 - val_loss: 276.5581 - val_mae: 276.5581 - learning_rate: 0.0020\n",
            "Epoch 2/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 224.4081 - mae: 224.4081 - val_loss: 247.0326 - val_mae: 247.0326 - learning_rate: 0.0020\n",
            "Epoch 3/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 205.7977 - mae: 205.7977 - val_loss: 221.0428 - val_mae: 221.0428 - learning_rate: 0.0020\n",
            "Epoch 4/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 199.7490 - mae: 199.7490 - val_loss: 207.7565 - val_mae: 207.7565 - learning_rate: 0.0020\n",
            "Epoch 5/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 196.9449 - mae: 196.9449 - val_loss: 195.9794 - val_mae: 195.9794 - learning_rate: 0.0020\n",
            "Epoch 6/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 197.2208 - mae: 197.2208 - val_loss: 170.3459 - val_mae: 170.3459 - learning_rate: 0.0020\n",
            "Epoch 7/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 185.8034 - mae: 185.8034 - val_loss: 171.2286 - val_mae: 171.2286 - learning_rate: 0.0020\n",
            "Epoch 8/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 182.8417 - mae: 182.8417 - val_loss: 173.5414 - val_mae: 173.5414 - learning_rate: 0.0020\n",
            "Epoch 9/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 196.0818 - mae: 196.0818 - val_loss: 171.6861 - val_mae: 171.6861 - learning_rate: 0.0020\n",
            "Epoch 10/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 191.2548 - mae: 191.2548 - val_loss: 167.1310 - val_mae: 167.1310 - learning_rate: 0.0020\n",
            "Epoch 11/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 198.5801 - mae: 198.5801 - val_loss: 175.7271 - val_mae: 175.7271 - learning_rate: 0.0020\n",
            "Epoch 12/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 191.9517 - mae: 191.9517 - val_loss: 173.1305 - val_mae: 173.1305 - learning_rate: 0.0020\n",
            "Epoch 13/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 184.1192 - mae: 184.1192 - val_loss: 184.6775 - val_mae: 184.6775 - learning_rate: 0.0020\n",
            "Epoch 14/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 189.2801 - mae: 189.2801 - val_loss: 186.5336 - val_mae: 186.5336 - learning_rate: 0.0020\n",
            "Epoch 15/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 176.3921 - mae: 176.3921 - val_loss: 212.4624 - val_mae: 212.4624 - learning_rate: 0.0020\n",
            "Epoch 16/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 183.4227 - mae: 183.4227 - val_loss: 164.2988 - val_mae: 164.2988 - learning_rate: 0.0020\n",
            "Epoch 17/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 187.8641 - mae: 187.8641 - val_loss: 159.6077 - val_mae: 159.6077 - learning_rate: 0.0020\n",
            "Epoch 18/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 185.6342 - mae: 185.6342 - val_loss: 172.8327 - val_mae: 172.8327 - learning_rate: 0.0020\n",
            "Epoch 19/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 184.6737 - mae: 184.6737 - val_loss: 192.1484 - val_mae: 192.1484 - learning_rate: 0.0020\n",
            "Epoch 20/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 189.6024 - mae: 189.6024 - val_loss: 170.0685 - val_mae: 170.0685 - learning_rate: 0.0020\n",
            "Epoch 21/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 184.6725 - mae: 184.6725 - val_loss: 172.0149 - val_mae: 172.0149 - learning_rate: 0.0020\n",
            "Epoch 22/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 183.9353 - mae: 183.9353 - val_loss: 164.8569 - val_mae: 164.8569 - learning_rate: 0.0020\n",
            "Epoch 23/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 177.8366 - mae: 177.8366 - val_loss: 178.1097 - val_mae: 178.1097 - learning_rate: 0.0020\n",
            "Epoch 24/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 182.6062 - mae: 182.6062 - val_loss: 169.0493 - val_mae: 169.0493 - learning_rate: 0.0020\n",
            "Epoch 25/80\n",
            "\u001b[1m129/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 183.3110 - mae: 183.3110\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 183.0145 - mae: 183.0145 - val_loss: 160.8444 - val_mae: 160.8444 - learning_rate: 0.0020\n",
            "Epoch 26/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 174.0308 - mae: 174.0308 - val_loss: 155.9794 - val_mae: 155.9794 - learning_rate: 0.0010\n",
            "Epoch 27/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 167.9762 - mae: 167.9762 - val_loss: 154.9485 - val_mae: 154.9485 - learning_rate: 0.0010\n",
            "Epoch 28/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 168.7091 - mae: 168.7091 - val_loss: 154.2449 - val_mae: 154.2449 - learning_rate: 0.0010\n",
            "Epoch 29/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 183.7716 - mae: 183.7716 - val_loss: 156.2879 - val_mae: 156.2879 - learning_rate: 0.0010\n",
            "Epoch 30/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 171.6065 - mae: 171.6065 - val_loss: 154.9071 - val_mae: 154.9071 - learning_rate: 0.0010\n",
            "Epoch 31/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 174.6328 - mae: 174.6328 - val_loss: 157.7967 - val_mae: 157.7967 - learning_rate: 0.0010\n",
            "Epoch 32/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 174.4115 - mae: 174.4115 - val_loss: 156.0159 - val_mae: 156.0159 - learning_rate: 0.0010\n",
            "Epoch 33/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 176.8758 - mae: 176.8758 - val_loss: 149.0616 - val_mae: 149.0616 - learning_rate: 0.0010\n",
            "Epoch 34/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 170.0833 - mae: 170.0833 - val_loss: 154.1120 - val_mae: 154.1120 - learning_rate: 0.0010\n",
            "Epoch 35/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 171.2164 - mae: 171.2164 - val_loss: 156.1608 - val_mae: 156.1608 - learning_rate: 0.0010\n",
            "Epoch 36/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 171.3413 - mae: 171.3413 - val_loss: 152.7258 - val_mae: 152.7258 - learning_rate: 0.0010\n",
            "Epoch 37/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 178.1758 - mae: 178.1758 - val_loss: 153.6034 - val_mae: 153.6034 - learning_rate: 0.0010\n",
            "Epoch 38/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 171.7475 - mae: 171.7475 - val_loss: 164.2786 - val_mae: 164.2786 - learning_rate: 0.0010\n",
            "Epoch 39/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 164.5140 - mae: 164.5140 - val_loss: 151.4895 - val_mae: 151.4895 - learning_rate: 0.0010\n",
            "Epoch 40/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 171.3053 - mae: 171.3053 - val_loss: 152.3557 - val_mae: 152.3557 - learning_rate: 0.0010\n",
            "Epoch 41/80\n",
            "\u001b[1m133/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 171.8862 - mae: 171.8862\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 171.8357 - mae: 171.8357 - val_loss: 157.8605 - val_mae: 157.8605 - learning_rate: 0.0010\n",
            "Epoch 42/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 172.5912 - mae: 172.5912 - val_loss: 150.7202 - val_mae: 150.7202 - learning_rate: 5.0000e-04\n",
            "Epoch 43/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 173.6516 - mae: 173.6516 - val_loss: 150.8098 - val_mae: 150.8098 - learning_rate: 5.0000e-04\n",
            "Epoch 44/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 163.3897 - mae: 163.3897 - val_loss: 148.4839 - val_mae: 148.4839 - learning_rate: 5.0000e-04\n",
            "Epoch 45/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 169.2176 - mae: 169.2176 - val_loss: 149.5358 - val_mae: 149.5358 - learning_rate: 5.0000e-04\n",
            "Epoch 46/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 165.1124 - mae: 165.1124 - val_loss: 148.9188 - val_mae: 148.9188 - learning_rate: 5.0000e-04\n",
            "Epoch 47/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 171.8849 - mae: 171.8849 - val_loss: 151.1318 - val_mae: 151.1318 - learning_rate: 5.0000e-04\n",
            "Epoch 48/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 171.3392 - mae: 171.3392 - val_loss: 150.5211 - val_mae: 150.5211 - learning_rate: 5.0000e-04\n",
            "Epoch 49/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 162.6660 - mae: 162.6660 - val_loss: 158.7859 - val_mae: 158.7859 - learning_rate: 5.0000e-04\n",
            "Epoch 50/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 169.6487 - mae: 169.6487 - val_loss: 160.7670 - val_mae: 160.7670 - learning_rate: 5.0000e-04\n",
            "Epoch 51/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 161.7777 - mae: 161.7777 - val_loss: 152.5321 - val_mae: 152.5321 - learning_rate: 5.0000e-04\n",
            "Epoch 52/80\n",
            "\u001b[1m127/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 162.8563 - mae: 162.8563\n",
            "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 162.9116 - mae: 162.9116 - val_loss: 154.2231 - val_mae: 154.2231 - learning_rate: 5.0000e-04\n",
            "Epoch 53/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 166.4655 - mae: 166.4655 - val_loss: 151.4569 - val_mae: 151.4569 - learning_rate: 2.5000e-04\n",
            "Epoch 54/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 158.3241 - mae: 158.3241 - val_loss: 152.8529 - val_mae: 152.8529 - learning_rate: 2.5000e-04\n",
            "Epoch 55/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 170.4486 - mae: 170.4486 - val_loss: 155.1752 - val_mae: 155.1752 - learning_rate: 2.5000e-04\n",
            "Epoch 56/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 163.9268 - mae: 163.9268 - val_loss: 151.7061 - val_mae: 151.7061 - learning_rate: 2.5000e-04\n",
            "Epoch 57/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 157.6000 - mae: 157.6000 - val_loss: 151.1865 - val_mae: 151.1865 - learning_rate: 2.5000e-04\n",
            "Epoch 58/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 166.5899 - mae: 166.5899 - val_loss: 149.1600 - val_mae: 149.1600 - learning_rate: 2.5000e-04\n",
            "Epoch 59/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 170.3949 - mae: 170.3949 - val_loss: 153.3829 - val_mae: 153.3829 - learning_rate: 2.5000e-04\n",
            "Epoch 60/80\n",
            "\u001b[1m133/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 165.0124 - mae: 165.0124\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 164.9110 - mae: 164.9110 - val_loss: 149.6408 - val_mae: 149.6408 - learning_rate: 2.5000e-04\n",
            "Epoch 61/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 160.9948 - mae: 160.9948 - val_loss: 149.0221 - val_mae: 149.0221 - learning_rate: 1.2500e-04\n",
            "Epoch 62/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 153.3100 - mae: 153.3100 - val_loss: 148.1715 - val_mae: 148.1715 - learning_rate: 1.2500e-04\n",
            "Epoch 63/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 168.0234 - mae: 168.0234 - val_loss: 150.7982 - val_mae: 150.7982 - learning_rate: 1.2500e-04\n",
            "Epoch 64/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 166.4729 - mae: 166.4729 - val_loss: 148.9537 - val_mae: 148.9537 - learning_rate: 1.2500e-04\n",
            "Epoch 65/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 165.6131 - mae: 165.6131 - val_loss: 148.1693 - val_mae: 148.1693 - learning_rate: 1.2500e-04\n",
            "Epoch 66/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 167.1344 - mae: 167.1344 - val_loss: 148.2915 - val_mae: 148.2915 - learning_rate: 1.2500e-04\n",
            "Epoch 67/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 162.9151 - mae: 162.9151 - val_loss: 149.2122 - val_mae: 149.2122 - learning_rate: 1.2500e-04\n",
            "Epoch 68/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 169.8510 - mae: 169.8510 - val_loss: 148.4907 - val_mae: 148.4907 - learning_rate: 1.2500e-04\n",
            "Epoch 69/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 162.7557 - mae: 162.7557 - val_loss: 148.1162 - val_mae: 148.1162 - learning_rate: 1.2500e-04\n",
            "Epoch 70/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 162.9049 - mae: 162.9049 - val_loss: 148.7232 - val_mae: 148.7232 - learning_rate: 1.2500e-04\n",
            "Epoch 71/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 172.6086 - mae: 172.6086 - val_loss: 149.3696 - val_mae: 149.3696 - learning_rate: 1.2500e-04\n",
            "Epoch 72/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 164.6308 - mae: 164.6308 - val_loss: 149.3358 - val_mae: 149.3358 - learning_rate: 1.2500e-04\n",
            "Epoch 73/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 163.2334 - mae: 163.2334 - val_loss: 149.1080 - val_mae: 149.1080 - learning_rate: 1.2500e-04\n",
            "Epoch 74/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 167.2623 - mae: 167.2623 - val_loss: 149.1855 - val_mae: 149.1855 - learning_rate: 1.2500e-04\n",
            "Epoch 75/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 159.6393 - mae: 159.6393 - val_loss: 148.7380 - val_mae: 148.7380 - learning_rate: 1.2500e-04\n",
            "Epoch 76/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 167.5354 - mae: 167.5354 - val_loss: 148.4420 - val_mae: 148.4420 - learning_rate: 1.2500e-04\n",
            "Epoch 77/80\n",
            "\u001b[1m126/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 160.2668 - mae: 160.2668\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 159.9763 - mae: 159.9763 - val_loss: 149.0898 - val_mae: 149.0898 - learning_rate: 1.2500e-04\n",
            "Epoch 78/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 159.3511 - mae: 159.3511 - val_loss: 148.7890 - val_mae: 148.7890 - learning_rate: 6.2500e-05\n",
            "Epoch 79/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 153.2269 - mae: 153.2269 - val_loss: 148.4240 - val_mae: 148.4240 - learning_rate: 6.2500e-05\n",
            "Epoch 80/80\n",
            "\u001b[1m135/135\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 160.7647 - mae: 160.7647 - val_loss: 148.6091 - val_mae: 148.6091 - learning_rate: 6.2500e-05\n",
            "Restoring model weights from the end of the best epoch: 69.\n",
            "Training gradient boosting ensemble...\n",
            "\n",
            "  Best ensemble weight: 0.6 NN / 0.4 GB\n",
            "\n",
            "==================================================\n",
            "Validation Results (DELTA prediction):\n",
            "  Delta MAE - Temp: 279.05, Tint: 4.79\n",
            "\n",
            "Validation Results (ABSOLUTE values):\n",
            "  Temperature MAE: 277.83\n",
            "  Tint MAE: 4.79\n",
            "  Average MAE: 141.31\n",
            "==================================================\n",
            "\n",
            "\n",
            "Generating predictions...\n",
            "Using currTemp/currTint for delta reconstruction\n",
            "Loading validation images...\n",
            "Loaded 100/493 images\n",
            "Loaded 200/493 images\n",
            "Loaded 300/493 images\n",
            "Loaded 400/493 images\n",
            "Generating predictions...\n",
            "\n",
            "Delta statistics (predicted):\n",
            "  Temp delta mean: -139.4, std: 1122.9\n",
            "  Tint delta mean: 0.3, std: 4.9\n",
            "\n",
            "Predictions saved to submission.csv\n",
            "\n",
            "Submission preview:\n",
            "                              id_global  Temperature  Tint\n",
            "0  EB5BEE31-8D4F-450A-8BDD-27C762C75AA6         5753    10\n",
            "1  DE666E1F-0433-4958-AEC0-9A0CC0F81036         5849    11\n",
            "2  F6A6EA9C-A5C2-4BBA-9812-5CE52B818CB6         6752    11\n",
            "3  BCC39DEF-598C-491A-A3CA-14A249717F36         5538    11\n",
            "4  390ED94E-0066-4822-99B9-8F1568BDFBF5         5938    10\n",
            "5  4577FF1A-9D78-403E-939E-76A3D9893757         5446    10\n",
            "6  6523B2F7-4E3B-41A1-B35D-D550857AC1C5         3662     9\n",
            "7  4AA9F823-799F-4B28-AF63-2C582D8C6806         3795    11\n",
            "8  8B6439B2-38EE-458C-9FF6-92712B83E524         3679    10\n",
            "9  95B5422D-7FCB-4676-AAA3-BFA1C675D888         4239    12\n",
            "\n",
            "Submission shape: (493, 3)\n",
            "\n",
            "Temperature range: [2000, 10000]\n",
            "Tint range: [-12, 28]\n",
            "\n",
            "âœ… Done! Upload submission.csv to the competition platform.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class WhiteBalancePredictor:\n",
        "    \"\"\"\n",
        "    Delta-based predictor: Predict CHANGE from current values\n",
        "    Key insight: We predict Temperature - currTemp and Tint - currTint\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.image_model = None\n",
        "        self.metadata_model = None\n",
        "        self.ensemble_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.feature_cols = None\n",
        "        self.feature_means = {}\n",
        "        self.feature_medians = {}\n",
        "        self.predict_deltas = True  # Predict changes instead of absolute values\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path='/content/aftershoot_extracted/dataset/Train'):\n",
        "        \"\"\"Load images and metadata with delta-based targets\"\"\"\n",
        "        sliders_df = pd.read_csv(f'{train_path}/sliders.csv')\n",
        "\n",
        "        # Exclude features\n",
        "        exclude_features = [\n",
        "            'id_global', 'Temperature', 'Tint',\n",
        "            'touchTime', 'editTime', 'lastEditTime',\n",
        "            'timestamp', 'date', 'time',\n",
        "            'copyCreationTime', 'captureTime', 'grayscale'\n",
        "        ]\n",
        "\n",
        "        all_features = [col for col in sliders_df.columns\n",
        "                       if col not in exclude_features]\n",
        "\n",
        "        self.feature_cols = []\n",
        "        for col in all_features:\n",
        "            if sliders_df[col].dtype in ['float64', 'int64']:\n",
        "                if sliders_df[col].std() > 1e-6:\n",
        "                    self.feature_cols.append(col)\n",
        "            elif sliders_df[col].dtype == 'object':\n",
        "                unique_vals = sliders_df[col].nunique()\n",
        "                if 1 < unique_vals < 100:\n",
        "                    self.feature_cols.append(col)\n",
        "\n",
        "        print(f\"Using {len(self.feature_cols)} features: {self.feature_cols}\")\n",
        "\n",
        "        X_meta = sliders_df[self.feature_cols].copy()\n",
        "\n",
        "        # KEY INSIGHT: Predict deltas (changes) instead of absolute values\n",
        "        if 'currTemp' in self.feature_cols and 'currTint' in self.feature_cols:\n",
        "            print(\"\\nðŸŽ¯ Using DELTA prediction (change from current values)\")\n",
        "\n",
        "            # Calculate deltas\n",
        "            temp_delta = sliders_df['Temperature'] - sliders_df['currTemp']\n",
        "            tint_delta = sliders_df['Tint'] - sliders_df['currTint']\n",
        "\n",
        "            # Remove extreme outliers (likely data errors)\n",
        "            temp_delta_q99 = temp_delta.quantile(0.99)\n",
        "            temp_delta_q01 = temp_delta.quantile(0.01)\n",
        "            tint_delta_q99 = tint_delta.quantile(0.99)\n",
        "            tint_delta_q01 = tint_delta.quantile(0.01)\n",
        "\n",
        "            print(f\"  Removing outliers outside: Temp [{temp_delta_q01:.0f}, {temp_delta_q99:.0f}], Tint [{tint_delta_q01:.0f}, {tint_delta_q99:.0f}]\")\n",
        "\n",
        "            # Clip extreme values\n",
        "            temp_delta = temp_delta.clip(temp_delta_q01, temp_delta_q99)\n",
        "            tint_delta = tint_delta.clip(tint_delta_q01, tint_delta_q99)\n",
        "\n",
        "            y = pd.DataFrame({\n",
        "                'temp_delta': temp_delta,\n",
        "                'tint_delta': tint_delta\n",
        "            })\n",
        "\n",
        "            # Store current values and clipping bounds\n",
        "            self.current_temp = sliders_df['currTemp'].copy()\n",
        "            self.current_tint = sliders_df['currTint'].copy()\n",
        "            self.temp_delta_min = temp_delta_q01\n",
        "            self.temp_delta_max = temp_delta_q99\n",
        "            self.tint_delta_min = tint_delta_q01\n",
        "            self.tint_delta_max = tint_delta_q99\n",
        "        else:\n",
        "            print(\"\\nâš ï¸  currTemp/currTint not found, using absolute prediction\")\n",
        "            self.predict_deltas = False\n",
        "            y = sliders_df[['Temperature', 'Tint']].copy()\n",
        "\n",
        "        ids = sliders_df['id_global'].values\n",
        "\n",
        "        # Store statistics\n",
        "        for col in X_meta.columns:\n",
        "            if X_meta[col].dtype in ['float64', 'int64']:\n",
        "                self.feature_means[col] = X_meta[col].mean()\n",
        "                self.feature_medians[col] = X_meta[col].median()\n",
        "\n",
        "        # Encode categorical features\n",
        "        categorical_cols = X_meta.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            self.label_encoders[col] = LabelEncoder()\n",
        "            X_meta[col] = self.label_encoders[col].fit_transform(X_meta[col].astype(str))\n",
        "            self.feature_means[col] = X_meta[col].mode()[0] if len(X_meta[col].mode()) > 0 else 0\n",
        "\n",
        "        # Load images\n",
        "        print(\"Loading images...\")\n",
        "        images = []\n",
        "        valid_indices = []\n",
        "\n",
        "        sample_id = ids[0]\n",
        "        possible_extensions = ['.tiff', '.tif', '.TIFF', '.TIF', '.jpg', '.JPG', '.png', '.PNG']\n",
        "        actual_extension = None\n",
        "\n",
        "        for ext in possible_extensions:\n",
        "            if os.path.exists(f'{train_path}/images/{sample_id}{ext}'):\n",
        "                actual_extension = ext\n",
        "                print(f\"Detected image extension: {ext}\")\n",
        "                break\n",
        "\n",
        "        if actual_extension is None:\n",
        "            raise ValueError(\"Could not detect image extension\")\n",
        "\n",
        "        for idx, img_id in enumerate(ids):\n",
        "            img_path = f'{train_path}/images/{img_id}{actual_extension}'\n",
        "            if os.path.exists(img_path):\n",
        "                try:\n",
        "                    img = Image.open(img_path)\n",
        "                    if img.mode != 'RGB':\n",
        "                        img = img.convert('RGB')\n",
        "                    img = img.resize((224, 224))\n",
        "                    img_array = np.array(img).astype(np.float32) / 255.0\n",
        "                    images.append(img_array)\n",
        "                    valid_indices.append(idx)\n",
        "\n",
        "                    if (idx + 1) % 500 == 0:\n",
        "                        print(f\"Loaded {idx + 1}/{len(ids)} images\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {img_id}: {e}\")\n",
        "\n",
        "        images = np.array(images)\n",
        "        X_meta = X_meta.iloc[valid_indices].reset_index(drop=True)\n",
        "        y = y.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "        if self.predict_deltas:\n",
        "            self.current_temp = self.current_temp.iloc[valid_indices].reset_index(drop=True)\n",
        "            self.current_tint = self.current_tint.iloc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Successfully loaded {len(images)} images\")\n",
        "\n",
        "        if self.predict_deltas:\n",
        "            print(f\"\\nDelta statistics:\")\n",
        "            print(f\"  Temp delta range: [{y['temp_delta'].min():.0f}, {y['temp_delta'].max():.0f}]\")\n",
        "            print(f\"  Temp delta mean: {y['temp_delta'].mean():.1f}, std: {y['temp_delta'].std():.1f}\")\n",
        "            print(f\"  Tint delta range: [{y['tint_delta'].min():.0f}, {y['tint_delta'].max():.0f}]\")\n",
        "            print(f\"  Tint delta mean: {y['tint_delta'].mean():.1f}, std: {y['tint_delta'].std():.1f}\")\n",
        "\n",
        "        return images, X_meta, y, ids[valid_indices]\n",
        "\n",
        "    def build_image_model(self, input_shape=(224, 224, 3)):\n",
        "        \"\"\"Lightweight CNN for image features\"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Input(shape=input_shape),\n",
        "\n",
        "            layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(32, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling2D(2),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling2D(2),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.4),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "        ])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_fusion_model(self, image_features_dim, metadata_dim):\n",
        "        \"\"\"Fusion model optimized for delta prediction\"\"\"\n",
        "        # Image branch\n",
        "        image_input = layers.Input(shape=(image_features_dim,))\n",
        "        x_img = layers.Dense(256, activation='relu')(image_input)\n",
        "        x_img = layers.BatchNormalization()(x_img)\n",
        "        x_img = layers.Dropout(0.3)(x_img)\n",
        "\n",
        "        # Metadata branch (currTemp/currTint are important here!)\n",
        "        metadata_input = layers.Input(shape=(metadata_dim,))\n",
        "        x_meta = layers.Dense(128, activation='relu')(metadata_input)\n",
        "        x_meta = layers.BatchNormalization()(x_meta)\n",
        "        x_meta = layers.Dropout(0.3)(x_meta)\n",
        "        x_meta = layers.Dense(64, activation='relu')(x_meta)\n",
        "\n",
        "        # Fusion\n",
        "        combined = layers.concatenate([x_img, x_meta])\n",
        "\n",
        "        x = layers.Dense(256, activation='relu')(combined)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        # Separate heads for temp_delta and tint_delta\n",
        "        temp_head = layers.Dense(64, activation='relu')(x)\n",
        "        temp_head = layers.Dropout(0.2)(temp_head)\n",
        "        temp_output = layers.Dense(1, name='temp_delta')(temp_head)\n",
        "\n",
        "        tint_head = layers.Dense(64, activation='relu')(x)\n",
        "        tint_head = layers.Dropout(0.2)(tint_head)\n",
        "        tint_output = layers.Dense(1, name='tint_delta')(tint_head)\n",
        "\n",
        "        output = layers.concatenate([temp_output, tint_output])\n",
        "\n",
        "        model = keras.Model(inputs=[image_input, metadata_input], outputs=output)\n",
        "        return model\n",
        "\n",
        "    def train(self, train_path='/content/aftershoot_extracted/dataset/Train',\n",
        "              epochs=80, batch_size=32):\n",
        "        \"\"\"Train with delta-based approach\"\"\"\n",
        "        images, X_meta, y, ids = self.load_and_preprocess_data(train_path)\n",
        "\n",
        "        # Split data\n",
        "        if self.predict_deltas:\n",
        "            X_img_train, X_img_val, X_meta_train, X_meta_val, y_train, y_val, curr_temp_train, curr_temp_val, curr_tint_train, curr_tint_val = train_test_split(\n",
        "                images, X_meta, y, self.current_temp, self.current_tint,\n",
        "                test_size=0.15, random_state=42\n",
        "            )\n",
        "        else:\n",
        "            X_img_train, X_img_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
        "                images, X_meta, y, test_size=0.15, random_state=42\n",
        "            )\n",
        "\n",
        "        # Scale metadata\n",
        "        X_meta_train_scaled = self.scaler.fit_transform(X_meta_train)\n",
        "        X_meta_val_scaled = self.scaler.transform(X_meta_val)\n",
        "\n",
        "        print(\"Training image feature extractor...\")\n",
        "        self.image_model = self.build_image_model()\n",
        "\n",
        "        print(\"Extracting image features...\")\n",
        "        img_features_train = self.image_model.predict(X_img_train, batch_size=batch_size, verbose=0)\n",
        "        img_features_val = self.image_model.predict(X_img_val, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        print(\"Training fusion model...\")\n",
        "        self.ensemble_model = self.build_fusion_model(\n",
        "            img_features_train.shape[1],\n",
        "            X_meta_train_scaled.shape[1]\n",
        "        )\n",
        "\n",
        "        # Loss function - MAE is good for deltas\n",
        "        self.ensemble_model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.002),\n",
        "            loss='mae',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        early_stop = keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=20, restore_best_weights=True, verbose=1\n",
        "        )\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=1\n",
        "        )\n",
        "\n",
        "        history = self.ensemble_model.fit(\n",
        "            [img_features_train, X_meta_train_scaled],\n",
        "            y_train.values,\n",
        "            validation_data=([img_features_val, X_meta_val_scaled], y_val.values),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Train gradient boosting\n",
        "        print(\"Training gradient boosting ensemble...\")\n",
        "        combined_features_train = np.concatenate([img_features_train, X_meta_train_scaled], axis=1)\n",
        "\n",
        "        self.metadata_model = MultiOutputRegressor(\n",
        "            GradientBoostingRegressor(\n",
        "                n_estimators=300,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.8,\n",
        "                random_state=42\n",
        "            )\n",
        "        )\n",
        "        self.metadata_model.fit(combined_features_train, y_train)\n",
        "\n",
        "        # Evaluate both models separately to determine best weights\n",
        "        val_pred_nn = self.ensemble_model.predict([img_features_val, X_meta_val_scaled], verbose=0)\n",
        "        combined_features_val = np.concatenate([img_features_val, X_meta_val_scaled], axis=1)\n",
        "        val_pred_gb = self.metadata_model.predict(combined_features_val)\n",
        "\n",
        "        if self.predict_deltas:\n",
        "            # Test different ensemble weights\n",
        "            best_mae = float('inf')\n",
        "            best_weight = 0.5\n",
        "\n",
        "            for nn_weight in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "                val_pred_delta = nn_weight * val_pred_nn + (1 - nn_weight) * val_pred_gb\n",
        "\n",
        "                # Clip deltas\n",
        "                val_pred_delta[:, 0] = np.clip(val_pred_delta[:, 0], self.temp_delta_min, self.temp_delta_max)\n",
        "                val_pred_delta[:, 1] = np.clip(val_pred_delta[:, 1], self.tint_delta_min, self.tint_delta_max)\n",
        "\n",
        "                # Convert to absolute\n",
        "                val_pred_absolute = np.column_stack([\n",
        "                    curr_temp_val.values + val_pred_delta[:, 0],\n",
        "                    curr_tint_val.values + val_pred_delta[:, 1]\n",
        "                ])\n",
        "\n",
        "                # Clip to valid ranges\n",
        "                val_pred_absolute[:, 0] = np.clip(val_pred_absolute[:, 0], 2000, 10000)\n",
        "                val_pred_absolute[:, 1] = np.clip(val_pred_absolute[:, 1], -150, 150)\n",
        "\n",
        "                # Get ground truth\n",
        "                y_val_absolute = np.column_stack([\n",
        "                    curr_temp_val.values + y_val.iloc[:, 0].values,\n",
        "                    curr_tint_val.values + y_val.iloc[:, 1].values\n",
        "                ])\n",
        "\n",
        "                mae_temp = np.mean(np.abs(y_val_absolute[:, 0] - val_pred_absolute[:, 0]))\n",
        "                mae_tint = np.mean(np.abs(y_val_absolute[:, 1] - val_pred_absolute[:, 1]))\n",
        "                avg_mae = (mae_temp + mae_tint) / 2\n",
        "\n",
        "                if avg_mae < best_mae:\n",
        "                    best_mae = avg_mae\n",
        "                    best_weight = nn_weight\n",
        "                    best_temp_mae = mae_temp\n",
        "                    best_tint_mae = mae_tint\n",
        "\n",
        "            self.nn_weight = best_weight\n",
        "            print(f\"\\n  Best ensemble weight: {best_weight:.1f} NN / {1-best_weight:.1f} GB\")\n",
        "\n",
        "            # Final predictions with best weight\n",
        "            val_pred_delta = best_weight * val_pred_nn + (1 - best_weight) * val_pred_gb\n",
        "            val_pred_delta[:, 0] = np.clip(val_pred_delta[:, 0], self.temp_delta_min, self.temp_delta_max)\n",
        "            val_pred_delta[:, 1] = np.clip(val_pred_delta[:, 1], self.tint_delta_min, self.tint_delta_max)\n",
        "\n",
        "            mae_temp = best_temp_mae\n",
        "            mae_tint = best_tint_mae\n",
        "\n",
        "            # Also show delta MAE\n",
        "            mae_temp_delta = np.mean(np.abs(y_val.iloc[:, 0].values - val_pred_delta[:, 0]))\n",
        "            mae_tint_delta = np.mean(np.abs(y_val.iloc[:, 1].values - val_pred_delta[:, 1]))\n",
        "\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Validation Results (DELTA prediction):\")\n",
        "            print(f\"  Delta MAE - Temp: {mae_temp_delta:.2f}, Tint: {mae_tint_delta:.2f}\")\n",
        "            print(f\"\\nValidation Results (ABSOLUTE values):\")\n",
        "            print(f\"  Temperature MAE: {mae_temp:.2f}\")\n",
        "            print(f\"  Tint MAE: {mae_tint:.2f}\")\n",
        "            print(f\"  Average MAE: {(mae_temp + mae_tint) / 2:.2f}\")\n",
        "            print(f\"{'='*50}\\n\")\n",
        "        else:\n",
        "            self.nn_weight = 0.5\n",
        "            val_pred_delta = 0.5 * val_pred_nn + 0.5 * val_pred_gb\n",
        "            mae_temp = np.mean(np.abs(y_val.iloc[:, 0].values - val_pred_delta[:, 0]))\n",
        "            mae_tint = np.mean(np.abs(y_val.iloc[:, 1].values - val_pred_delta[:, 1]))\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Temperature MAE: {mae_temp:.2f}\")\n",
        "            print(f\"Tint MAE: {mae_tint:.2f}\")\n",
        "            print(f\"Average MAE: {(mae_temp + mae_tint) / 2:.2f}\")\n",
        "            print(f\"{'='*50}\\n\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, val_path='/content/aftershoot_extracted/dataset/Validation',\n",
        "                output_file='submission.csv'):\n",
        "        \"\"\"Generate predictions\"\"\"\n",
        "        sliders_input = pd.read_csv(f'{val_path}/sliders_input.csv')\n",
        "        ids = sliders_input['id_global'].values\n",
        "\n",
        "        # Store current values for delta reconstruction\n",
        "        if self.predict_deltas:\n",
        "            if 'currTemp' in sliders_input.columns and 'currTint' in sliders_input.columns:\n",
        "                curr_temp_val = sliders_input['currTemp'].values\n",
        "                curr_tint_val = sliders_input['currTint'].values\n",
        "                print(f\"Using currTemp/currTint for delta reconstruction\")\n",
        "            else:\n",
        "                print(\"âš ï¸  Warning: currTemp/currTint not in validation, using defaults\")\n",
        "                curr_temp_val = np.full(len(ids), 5500)  # Typical daylight\n",
        "                curr_tint_val = np.full(len(ids), 0)\n",
        "\n",
        "        # Handle missing features\n",
        "        available_cols = [col for col in self.feature_cols if col in sliders_input.columns]\n",
        "        missing_cols = [col for col in self.feature_cols if col not in sliders_input.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "            print(f\"Warning: {len(missing_cols)} features missing\")\n",
        "\n",
        "        X_meta = sliders_input[available_cols].copy()\n",
        "\n",
        "        for col in missing_cols:\n",
        "            if col in self.feature_medians:\n",
        "                X_meta[col] = self.feature_medians[col]\n",
        "            elif col in self.feature_means:\n",
        "                X_meta[col] = self.feature_means[col]\n",
        "            else:\n",
        "                X_meta[col] = 0\n",
        "\n",
        "        X_meta = X_meta[self.feature_cols]\n",
        "\n",
        "        # Handle categorical features\n",
        "        for col in X_meta.select_dtypes(include=['object']).columns:\n",
        "            if col in self.label_encoders:\n",
        "                known_categories = set(self.label_encoders[col].classes_)\n",
        "                X_meta[col] = X_meta[col].astype(str)\n",
        "                mode_value = self.label_encoders[col].classes_[int(self.feature_means.get(col, 0))]\n",
        "                X_meta[col] = X_meta[col].apply(lambda x: x if x in known_categories else mode_value)\n",
        "                X_meta[col] = self.label_encoders[col].transform(X_meta[col])\n",
        "            else:\n",
        "                X_meta[col] = 0\n",
        "\n",
        "        # Load images\n",
        "        print(\"Loading validation images...\")\n",
        "        images = []\n",
        "\n",
        "        sample_id = ids[0]\n",
        "        possible_extensions = ['.tiff', '.tif', '.TIFF', '.TIF', '.jpg', '.JPG', '.png', '.PNG']\n",
        "        actual_extension = '.tiff'\n",
        "\n",
        "        for ext in possible_extensions:\n",
        "            if os.path.exists(f'{val_path}/images/{sample_id}{ext}'):\n",
        "                actual_extension = ext\n",
        "                break\n",
        "\n",
        "        for idx, img_id in enumerate(ids):\n",
        "            img_path = f'{val_path}/images/{img_id}{actual_extension}'\n",
        "            try:\n",
        "                img = Image.open(img_path)\n",
        "                if img.mode != 'RGB':\n",
        "                    img = img.convert('RGB')\n",
        "                img = img.resize((224, 224))\n",
        "                img_array = np.array(img).astype(np.float32) / 255.0\n",
        "                images.append(img_array)\n",
        "\n",
        "                if (idx + 1) % 100 == 0:\n",
        "                    print(f\"Loaded {idx + 1}/{len(ids)} images\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_id}: {e}\")\n",
        "                images.append(np.zeros((224, 224, 3), dtype=np.float32))\n",
        "\n",
        "        images = np.array(images)\n",
        "        X_meta_scaled = self.scaler.transform(X_meta)\n",
        "\n",
        "        # Predict\n",
        "        print(\"Generating predictions...\")\n",
        "        img_features = self.image_model.predict(images, batch_size=32, verbose=0)\n",
        "\n",
        "        pred_nn = self.ensemble_model.predict([img_features, X_meta_scaled], verbose=0)\n",
        "        combined_features = np.concatenate([img_features, X_meta_scaled], axis=1)\n",
        "        pred_gb = self.metadata_model.predict(combined_features)\n",
        "\n",
        "        # Use optimized ensemble weight\n",
        "        nn_weight = getattr(self, 'nn_weight', 0.5)\n",
        "        pred_delta = nn_weight * pred_nn + (1 - nn_weight) * pred_gb\n",
        "\n",
        "        # Clip predicted deltas to reasonable range\n",
        "        if self.predict_deltas:\n",
        "            pred_delta[:, 0] = np.clip(pred_delta[:, 0], self.temp_delta_min, self.temp_delta_max)\n",
        "            pred_delta[:, 1] = np.clip(pred_delta[:, 1], self.tint_delta_min, self.tint_delta_max)\n",
        "\n",
        "        # Convert deltas to absolute values\n",
        "        if self.predict_deltas:\n",
        "            predictions = np.column_stack([\n",
        "                curr_temp_val + pred_delta[:, 0],\n",
        "                curr_tint_val + pred_delta[:, 1]\n",
        "            ])\n",
        "\n",
        "            # Clip final predictions to valid ranges\n",
        "            predictions[:, 0] = np.clip(predictions[:, 0], 2000, 10000)  # Temperature: 2000-10000K\n",
        "            predictions[:, 1] = np.clip(predictions[:, 1], -150, 150)    # Tint: -150 to +150\n",
        "\n",
        "            print(f\"\\nDelta statistics (predicted):\")\n",
        "            print(f\"  Temp delta mean: {pred_delta[:, 0].mean():.1f}, std: {pred_delta[:, 0].std():.1f}\")\n",
        "            print(f\"  Tint delta mean: {pred_delta[:, 1].mean():.1f}, std: {pred_delta[:, 1].std():.1f}\")\n",
        "        else:\n",
        "            predictions = pred_delta\n",
        "            # Clip to valid ranges\n",
        "            predictions[:, 0] = np.clip(predictions[:, 0], 2000, 10000)\n",
        "            predictions[:, 1] = np.clip(predictions[:, 1], -150, 150)\n",
        "\n",
        "        predictions = np.round(predictions).astype(int)\n",
        "\n",
        "        # Create submission\n",
        "        submission = pd.DataFrame({\n",
        "            'id_global': ids,\n",
        "            'Temperature': predictions[:, 0],\n",
        "            'Tint': predictions[:, 1]\n",
        "        })\n",
        "\n",
        "        submission.to_csv(output_file, index=False)\n",
        "        print(f\"\\nPredictions saved to {output_file}\")\n",
        "\n",
        "        return submission\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    predictor = WhiteBalancePredictor()\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"WHITE BALANCE PREDICTOR - Delta-based approach\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    history = predictor.train(\n",
        "        train_path='/content/aftershoot_extracted/dataset/Train',\n",
        "        epochs=80,\n",
        "        batch_size=16\n",
        "    )\n",
        "\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "    submission = predictor.predict(\n",
        "        val_path='/content/aftershoot_extracted/dataset/Validation',\n",
        "        output_file='submission.csv'\n",
        "    )\n",
        "\n",
        "    print(\"\\nSubmission preview:\")\n",
        "    print(submission.head(10))\n",
        "    print(f\"\\nSubmission shape: {submission.shape}\")\n",
        "    print(f\"\\nTemperature range: [{submission['Temperature'].min()}, {submission['Temperature'].max()}]\")\n",
        "    print(f\"Tint range: [{submission['Tint'].min()}, {submission['Tint'].max()}]\")\n",
        "    print(\"\\nâœ… Done! Upload submission.csv to the competition platform.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iv40yetir-P2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}